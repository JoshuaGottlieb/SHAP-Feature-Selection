{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y2eTEqmkpV6m"
      },
      "outputs": [],
      "source": [
        "!pip -q install shap xgboost\n",
        "\n",
        "import os, sys, math, json, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "\n",
        "RS = 42  # global seed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Column typing & encoders"
      ],
      "metadata": {
        "id": "RB3JopJB1RkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/sample_data/processed.cleveland.data\"\n",
        "\n",
        "if not os.path.exists(PATH):\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"Upload processed.cleveland.data\")\n",
        "        up = files.upload()\n",
        "        # File will land in /content with its original name\n",
        "        if \"processed.cleveland.data\" not in up:\n",
        "            raise RuntimeError(\"Please upload 'processed.cleveland.data'.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Couldn't find or upload 'processed.cleveland.data'\") from e"
      ],
      "metadata": {
        "id": "IGs1RwD62yfo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) Read & basic clean"
      ],
      "metadata": {
        "id": "QYA_C-Dw1lWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\n",
        "    'age','sex','cp','trestbps','chol','fbs','restecg',\n",
        "    'thalach','exang','oldpeak','slope','ca','thal','target'\n",
        "]\n",
        "df = pd.read_csv(PATH, header=None, names=cols, na_values=[\"?\"])\n",
        "# Convert to numeric (safeguard)\n",
        "for c in cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Binary target: 0 = no disease, 1 = disease (1..4 → 1)\n",
        "df['target'] = (df['target'] > 0).astype(int)\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(df.head())\n",
        "print(\"\\nMissing counts:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx800zeJqEZk",
        "outputId": "e5f642e3-5cb7-40be-ca45-071513d23936"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (303, 14)\n",
            "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
            "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
            "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
            "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
            "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
            "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
            "\n",
            "   slope   ca  thal  target  \n",
            "0    3.0  0.0   6.0       0  \n",
            "1    2.0  3.0   3.0       1  \n",
            "2    2.0  2.0   7.0       1  \n",
            "3    3.0  0.0   3.0       0  \n",
            "4    1.0  0.0   3.0       0  \n",
            "\n",
            "Missing counts:\n",
            " age         0\n",
            "sex         0\n",
            "cp          0\n",
            "trestbps    0\n",
            "chol        0\n",
            "fbs         0\n",
            "restecg     0\n",
            "thalach     0\n",
            "exang       0\n",
            "oldpeak     0\n",
            "slope       0\n",
            "ca          4\n",
            "thal        2\n",
            "target      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Define features & preprocessing"
      ],
      "metadata": {
        "id": "VhC86ZGC1pjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numeric vs categorical (these are coded integers but categorical by meaning)\n",
        "cat_cols = ['sex','cp','fbs','restecg','exang','slope','ca','thal']\n",
        "num_cols = [c for c in df.columns if c not in cat_cols + ['target']]\n",
        "\n",
        "numeric_pipe = SimpleImputer(strategy=\"median\")\n",
        "categorical_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_pipe, num_cols),\n",
        "        (\"cat\", categorical_pipe, cat_cols),\n",
        "    ],\n",
        "    sparse_threshold=1.0\n",
        ")\n",
        "\n",
        "X = df.drop(columns=[\"target\"])\n",
        "y = df[\"target\"]\n",
        "\n",
        "# Stratified split (fixed seed)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=RS\n",
        ")\n",
        "\n",
        "# Inner split from TRAIN only for picking K (avoid test leakage)\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.25, stratify=y_train, random_state=RS\n",
        ")\n",
        "# So: 60% train, 20% val, 20% test\n",
        "\n",
        "# Fit preprocess on TRAIN ONLY\n",
        "preprocess.fit(X_tr)\n",
        "Xtr_enc = preprocess.transform(X_tr)\n",
        "Xval_enc = preprocess.transform(X_val)\n",
        "Xte_enc  = preprocess.transform(X_test)\n",
        "\n",
        "try:\n",
        "    feat_names = preprocess.get_feature_names_out()\n",
        "except:\n",
        "    # Fallback (rare in newer sklearn)\n",
        "    feat_names = [f\"f_{i}\" for i in range(Xtr_enc.shape[1])]"
      ],
      "metadata": {
        "id": "mNssg7FvqcR7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4) Baseline models (FULL features)"
      ],
      "metadata": {
        "id": "5v9l3Ei31tqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from sklearn.preprocessing import TargetEncoder  # sklearn >= 1.6\n",
        "    _SKLEARN_TE = True\n",
        "except Exception:\n",
        "    _SKLEARN_TE = False\n",
        "    try:\n",
        "        # Colab-friendly fallback\n",
        "        import sys, subprocess\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"category_encoders\"], check=False)\n",
        "        from category_encoders import TargetEncoder as CAT_TargetEncoder\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            \"TargetEncoder not available. Upgrade scikit-learn to >=1.6 OR allow installing category_encoders.\"\n",
        "        )\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os"
      ],
      "metadata": {
        "id": "sMpn_-TR8pIh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Column typing & encoders\n",
        "\n",
        "def n_unique_nonnull(series):\n",
        "    return series.dropna().nunique()\n",
        "\n",
        "card = {c: n_unique_nonnull(X_train[c]) for c in cat_cols}\n",
        "\n",
        "bin_cols   = [c for c,v in card.items() if v == 2]\n",
        "low_cols   = [c for c,v in card.items() if 3 <= v <= 10]\n",
        "high_cols  = [c for c,v in card.items() if v > 10]\n",
        "\n",
        "# Pipelines per block\n",
        "num_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "bin_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
        "])\n",
        "\n",
        "low_ohe_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "\n",
        "if _SKLEARN_TE:\n",
        "    te_estimator = TargetEncoder  # sklearn TargetEncoder\n",
        "else:\n",
        "    te_estimator = CAT_TargetEncoder  # category_encoders fallback\n",
        "\n",
        "high_te_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"te\", te_estimator())\n",
        "])\n",
        "\n",
        "# Build the ColumnTransformer; force dense output overall\n",
        "transformers = []\n",
        "if num_cols:\n",
        "    transformers.append((\"num\", num_pipe, num_cols))\n",
        "if bin_cols:\n",
        "    transformers.append((\"bin\", bin_pipe, bin_cols))\n",
        "if low_cols:\n",
        "    transformers.append((\"low\", low_ohe_pipe, low_cols))\n",
        "if high_cols:\n",
        "    transformers.append((\"high\", high_te_pipe, high_cols))\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=transformers,\n",
        "    remainder=\"drop\",\n",
        "    sparse_threshold=0.0   # force dense\n",
        ")"
      ],
      "metadata": {
        "id": "sMHsjXj38wjU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Define base models\n",
        "\n",
        "models = {\n",
        "    \"logreg\": LogisticRegression(random_state=RS, solver=\"saga\", max_iter=3000),\n",
        "    \"dtree\":  DecisionTreeClassifier(random_state=RS),\n",
        "    \"rf\":     RandomForestClassifier(random_state=RS, n_jobs=-1),\n",
        "    \"xgb\":    XGBClassifier(\n",
        "                 random_state=RS,\n",
        "                 tree_method=\"hist\",\n",
        "                 # silent defaults; other params tuned in grid\n",
        "                 n_jobs=-1\n",
        "             ),\n",
        "    \"svc\":    SVC(random_state=RS, probability=True)\n",
        "}"
      ],
      "metadata": {
        "id": "uyt5JhqS82jF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Grids\n",
        "\n",
        "logreg_grid = {\n",
        "    'logreg__penalty': ['l2', 'l1'],\n",
        "    'logreg__C': [10**x for x in range(-2, 3)]\n",
        "}\n",
        "\n",
        "dtree_grid = {\n",
        "    'dtree__max_depth': [None] + [x for x in range(3, 11, 2)],\n",
        "    'dtree__min_samples_split': [x for x in range(2, 14, 4)],\n",
        "    'dtree__min_samples_leaf': [1, 10, 20]\n",
        "}\n",
        "\n",
        "rf_grid = {\n",
        "    'rf__n_estimators': [100, 200, 400, 600],\n",
        "    'rf__max_depth': [None] + [x for x in range(3, 11, 2)],\n",
        "    'rf__min_samples_split': [x for x in range(2, 14, 4)],\n",
        "    'rf__min_samples_leaf': [1, 10, 20]\n",
        "}\n",
        "\n",
        "xgb_grid = {\n",
        "    'xgb__n_estimators': [100, 200, 400],\n",
        "    'xgb__max_depth': [3, 4, 5],\n",
        "    'xgb__min_child_weight': [1, 3, 5],\n",
        "    'xgb__subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'xgb__colsample_bytree': [0.7, 0.8, 0.9, 1.0]  # NOTE: correct name is \"colsample_bytree\"\n",
        "}\n",
        "\n",
        "svc_grid = {\n",
        "    'svc__C': [10**x for x in range(-2, 3)],\n",
        "    'svc__gamma': [10**x for x in range(-2, 3)]\n",
        "}\n",
        "\n",
        "grids = {\n",
        "    \"logreg\": logreg_grid,\n",
        "    \"dtree\":  dtree_grid,\n",
        "    \"rf\":     rf_grid,\n",
        "    \"xgb\":    xgb_grid,\n",
        "    \"svc\":    svc_grid\n",
        "}"
      ],
      "metadata": {
        "id": "0uXlJAuJ86M-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Pipelines & Search\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        "import joblib\n",
        "from scipy.stats import randint, uniform\n",
        "import time\n",
        "\n",
        "# 1) Faster CV & scoring\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RS)  # 3 folds\n",
        "scoring = \"roc_auc\"\n",
        "\n",
        "# 2) Optional: cache preprocessing (BIG speedup)\n",
        "CACHE_DIR = \"/content/cache\"\n",
        "memory = joblib.Memory(CACHE_DIR, verbose=0)\n",
        "\n",
        "# 3) use gpu_hist for XGB\n",
        "xgb_tree_method = \"gpu_hist\" if \"CUDA_VISIBLE_DEVICES\" in os.environ else \"hist\"\n",
        "if hasattr(models[\"xgb\"], \"set_params\"):\n",
        "    models[\"xgb\"].set_params(tree_method=xgb_tree_method)\n",
        "\n",
        "dataset_name = \"cleveland-heart\"\n",
        "results = {}\n",
        "\n",
        "# 4) Lean search spaces\n",
        "logreg_grid = {\n",
        "    'logreg__penalty': ['l2', 'l1'],\n",
        "    'logreg__C': [0.01, 0.1, 1, 10, 100],\n",
        "}\n",
        "\n",
        "dtree_grid = {\n",
        "    'dtree__max_depth': [None, 3, 5, 7, 9],\n",
        "    'dtree__min_samples_split': [2, 6, 10],\n",
        "    'dtree__min_samples_leaf': [1, 5, 10],\n",
        "}\n",
        "\n",
        "# Randomized distributions for heavy models\n",
        "rf_space = {\n",
        "    'rf__n_estimators': randint(120, 301),            # 120–300\n",
        "    'rf__max_depth': [None, 5, 7, 9],\n",
        "    'rf__min_samples_split': [2, 6, 10],\n",
        "    'rf__min_samples_leaf': [1, 3, 5],\n",
        "}\n",
        "\n",
        "xgb_space = {\n",
        "    'xgb__n_estimators': randint(120, 241),           # 120–240\n",
        "    'xgb__max_depth': [3, 4, 5],\n",
        "    'xgb__min_child_weight': [1, 3, 5],\n",
        "    'xgb__subsample': [0.8, 1.0],\n",
        "    'xgb__colsample_bytree': [0.8, 1.0],\n",
        "    # tree_method already set above\n",
        "}\n",
        "\n",
        "svc_space = {\n",
        "    'svc__C': [0.1, 1, 10],\n",
        "    'svc__gamma': ['scale', 0.1, 0.01],\n",
        "}\n",
        "\n",
        "# 5) Decide which uses grid vs randomized\n",
        "search_plan = {\n",
        "    \"logreg\": (\"grid\", logreg_grid),\n",
        "    \"dtree\":  (\"grid\", dtree_grid),\n",
        "    \"rf\":     (\"random\", rf_space),\n",
        "    \"xgb\":    (\"random\", xgb_space),\n",
        "    \"svc\":    (\"random\", svc_space),\n",
        "}\n",
        "\n",
        "print(\"FAST MODE: starting searches...\\n\")\n",
        "\n",
        "for name, estimator in models.items():\n",
        "    # Pipeline with caching\n",
        "    pipe = Pipeline(steps=[(\"prep\", preprocess), (name, estimator)], memory=memory)\n",
        "\n",
        "    kind, space = search_plan[name]\n",
        "    if kind == \"grid\":\n",
        "        searcher = GridSearchCV(\n",
        "            pipe, space, scoring=scoring, cv=cv, n_jobs=-1, refit=True, verbose=0\n",
        "        )\n",
        "    else:\n",
        "        # n_iter controls speed/quality tradeoff (try 15–30)\n",
        "        searcher = RandomizedSearchCV(\n",
        "            pipe, space, n_iter=20, scoring=scoring, cv=cv,\n",
        "            n_jobs=-1, refit=True, verbose=0, random_state=RS\n",
        "        )\n",
        "\n",
        "    t0 = time.time()\n",
        "    searcher.fit(X_train, y_train)  # TRAIN ONLY\n",
        "    fit_secs = time.time() - t0\n",
        "\n",
        "    # Evaluate on TEST\n",
        "    proba = searcher.predict_proba(X_test)[:, 1]\n",
        "    pred  = (proba >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, pred)\n",
        "    f1  = f1_score(y_test, pred)\n",
        "    auc = roc_auc_score(y_test, proba)\n",
        "\n",
        "    results[name] = {\n",
        "        \"best_params\": searcher.best_params_,\n",
        "        \"best_score_cv\": searcher.best_score_,\n",
        "        \"test_acc\": acc,\n",
        "        \"test_f1\": f1,\n",
        "        \"test_auc\": auc,\n",
        "        \"secs\": fit_secs,\n",
        "        \"model\": searcher\n",
        "    }\n",
        "\n",
        "    # Save each fitted searcher (pipeline included)\n",
        "    fname = f\"{dataset_name}-{name}-full.pickle\"\n",
        "    with open(fname, \"wb\") as f:\n",
        "        pickle.dump(searcher, f)\n",
        "\n",
        "    print(f\"{name.upper():5s} done in {fit_secs/60:.1f} min | \"\n",
        "          f\"AUC={auc:.3f} ACC={acc:.3f} F1={f1:.3f}\")\n",
        "\n",
        "print(\"\\n=== FAST MODE: Test Metrics ===\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k.upper():5s} | AUC={v['test_auc']:.3f}  ACC={v['test_acc']:.3f}  F1={v['test_f1']:.3f}  (CV best={v['best_score_cv']:.3f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UDr23Q18-8q",
        "outputId": "fedce144-c44b-46a8-862f-7e0108f468a8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAST MODE: starting searches...\n",
            "\n",
            "LOGREG done in 0.1 min | AUC=0.968 ACC=0.902 F1=0.897\n",
            "DTREE done in 0.1 min | AUC=0.871 ACC=0.869 F1=0.857\n",
            "RF    done in 0.4 min | AUC=0.956 ACC=0.902 F1=0.897\n",
            "XGB   done in 0.0 min | AUC=0.946 ACC=0.885 F1=0.881\n",
            "SVC   done in 0.0 min | AUC=0.966 ACC=0.869 F1=0.867\n",
            "\n",
            "=== FAST MODE: Test Metrics ===\n",
            "LOGREG | AUC=0.968  ACC=0.902  F1=0.897  (CV best=0.908)\n",
            "DTREE | AUC=0.871  ACC=0.869  F1=0.857  (CV best=0.850)\n",
            "RF    | AUC=0.956  ACC=0.902  F1=0.897  (CV best=0.901)\n",
            "XGB   | AUC=0.946  ACC=0.885  F1=0.881  (CV best=0.882)\n",
            "SVC   | AUC=0.966  ACC=0.869  F1=0.867  (CV best=0.900)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Report\n",
        "\n",
        "print(\"\\n=== Test Metrics (held-out TEST) ===\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k.upper():5s} | AUC={v['test_auc']:.3f}  ACC={v['test_acc']:.3f}  F1={v['test_f1']:.3f}  (CV best={v['best_score_cv']:.3f})\")\n",
        "\n",
        "print(\"\\nBest params per model:\")\n",
        "for k, v in results.items():\n",
        "    print(k, \"→\", v[\"best_params\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYbfJJh8q-ko",
        "outputId": "f63f9f6d-3937-4dec-99dc-3612c4beab6c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Metrics (held-out TEST) ===\n",
            "LOGREG | AUC=0.968  ACC=0.902  F1=0.897  (CV best=0.908)\n",
            "DTREE | AUC=0.871  ACC=0.869  F1=0.857  (CV best=0.850)\n",
            "RF    | AUC=0.956  ACC=0.902  F1=0.897  (CV best=0.901)\n",
            "XGB   | AUC=0.946  ACC=0.885  F1=0.881  (CV best=0.882)\n",
            "SVC   | AUC=0.966  ACC=0.869  F1=0.867  (CV best=0.900)\n",
            "\n",
            "Best params per model:\n",
            "logreg → {'logreg__C': 0.1, 'logreg__penalty': 'l2'}\n",
            "dtree → {'dtree__max_depth': 3, 'dtree__min_samples_leaf': 10, 'dtree__min_samples_split': 2}\n",
            "rf → {'rf__max_depth': None, 'rf__min_samples_leaf': 5, 'rf__min_samples_split': 10, 'rf__n_estimators': 200}\n",
            "xgb → {'xgb__colsample_bytree': 0.8, 'xgb__max_depth': 3, 'xgb__min_child_weight': 5, 'xgb__n_estimators': 137, 'xgb__subsample': 1.0}\n",
            "svc → {'svc__gamma': 0.01, 'svc__C': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Xx8gk_p-OOq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}