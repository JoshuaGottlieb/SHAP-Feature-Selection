{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f57e01-e5a1-4668-988a-fda31fd61187",
   "metadata": {},
   "source": [
    "## Load modules and set iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e6f68-d42c-4a33-8625-13e06be46cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "\n",
    "from modules.utils import load_object, load_dataset\n",
    "from modules.selection import feature_select, create_feature_selected_dataset\n",
    "from modules.training import fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56147259-d069-4e45-aa17-72bea98eb976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory paths for data, models, and SHAP explanations\n",
    "train_dir = '../data/processed/train'\n",
    "test_dir = '../data/processed/test'\n",
    "feature_select_dir = '../data/reduced'\n",
    "models_dir = '../models'\n",
    "explanations_dir = '../shap-explanations'\n",
    "\n",
    "# Retrieve and sort all available dataset names based on model directories\n",
    "dataset_names = sorted(os.listdir(models_dir))\n",
    "\n",
    "# Define feature selection aggregation strategies\n",
    "strategies = ['sum', 'max']\n",
    "\n",
    "# Thresholds for feature selection based on the 'sum' strategy\n",
    "# Cumulative importance thresholds for the 'sum' approach\n",
    "sum_thresholds = [0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "# Thresholds for feature selection based on the 'max'\n",
    "# Minimum SHAP value cutoffs for the 'max' approach\n",
    "max_thresholds = [0.01, 0.05, 0.1, 0.15]\n",
    "\n",
    "# List of model types to iterate over during retraining or evaluation\n",
    "model_types = ['logreg', 'dt', 'rf', 'xgb', 'svc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a76770-15a2-4a12-a5a8-4ad50c538516",
   "metadata": {},
   "source": [
    "## Generate reduced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6f586f-7596-4f58-a61d-8dba6469399b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate feature-selected datasets across datasets, models, and strategies\n",
    "for dataset in dataset_names:\n",
    "    set_name_snake = dataset.replace('-', '_')\n",
    "\n",
    "    print(f\"[INFO] Processing dataset: {dataset}\")\n",
    "\n",
    "    # Load processed train/test splits\n",
    "    train = load_dataset(os.path.join('../data/processed/train', f'{set_name_snake}-train.csv.gz'))\n",
    "    test = load_dataset(os.path.join('../data/processed/test', f'{set_name_snake}-test.csv.gz'))\n",
    "    print(f\"[INFO] Loaded training and testing data (train shape: {train.shape}, test shape: {test.shape})\")\n",
    "\n",
    "    # Iterate through model types (e.g., rf, xgb, etc.)\n",
    "    for model_type in model_types:\n",
    "        print(f\"[INFO] Model type: {model_type}\")\n",
    "        try:\n",
    "            # Load SHAP explanations for this dataset-model combination\n",
    "            shap_path = os.path.join(\n",
    "                explanations_dir,\n",
    "                dataset,\n",
    "                f'{set_name_snake}-{model_type}-sampling-global.pickle.xz'\n",
    "            )\n",
    "            shap_values = load_object(shap_path)['mean_shap']\n",
    "        except:\n",
    "            print(f\"[ERROR] Unable to load SHAP values from: {shap_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"[INFO] Loaded SHAP values from: {shap_path}\")\n",
    "        print(f\"[INFO] Number of SHAP values: {len(shap_values)}\")\n",
    "\n",
    "        # Loop through feature selection strategies\n",
    "        for strategy in strategies:\n",
    "            print(f\"[INFO] Strategy: {strategy}\")\n",
    "\n",
    "            # Strategy 1: Cumulative SHAP importance ('sum')\n",
    "            if strategy == 'sum':\n",
    "                for sum_threshold in sum_thresholds:\n",
    "                    print(f\"[INFO] Applying 'sum' strategy with threshold = {sum_threshold:.2f}\")\n",
    "                    \n",
    "                    # Select top features based on cumulative SHAP contribution\n",
    "                    idx = feature_select(\n",
    "                        shap_values = shap_values,\n",
    "                        kind = 'sum',\n",
    "                        sum_threshold = sum_threshold,\n",
    "                        # min_strength = 0.01   # Optional parameter to prune low value features\n",
    "                    )\n",
    "                    print(f\"[INFO] Selected {len(idx) - 1} features.\")\n",
    "\n",
    "                    # Save reduced datasets\n",
    "                    create_feature_selected_dataset(\n",
    "                        idx = idx,\n",
    "                        train = train,\n",
    "                        test = test,\n",
    "                        root_dir = feature_select_dir,\n",
    "                        dataset_name = set_name_snake,\n",
    "                        model_type = model_type,\n",
    "                        selection_strategy = strategy,\n",
    "                        selection_threshold = sum_threshold\n",
    "                    )\n",
    "                    print(f\"[DONE] Saved reduced datasets for 'sum' strategy ({sum_threshold:.2f}).\\n\")\n",
    "\n",
    "            # Strategy 2: Maximum SHAP threshold ('max')\n",
    "            elif strategy == 'max':\n",
    "                for max_threshold in max_thresholds:\n",
    "                    print(f\"[INFO] Applying 'max' strategy with threshold = {max_threshold:.2f}\")\n",
    "\n",
    "                    # Select top features based on relative max SHAP value\n",
    "                    idx = feature_select(\n",
    "                        shap_values = shap_values,\n",
    "                        kind = 'max',\n",
    "                        max_threshold = max_threshold\n",
    "                    )\n",
    "                    print(f\"[INFO] Selected {len(idx) - 1} features.\")\n",
    "\n",
    "                    # Save reduced datasets\n",
    "                    create_feature_selected_dataset(\n",
    "                        idx = idx,\n",
    "                        train = train,\n",
    "                        test = test,\n",
    "                        root_dir = feature_select_dir,\n",
    "                        dataset_name = set_name_snake,\n",
    "                        model_type = model_type,\n",
    "                        selection_strategy = strategy,\n",
    "                        selection_threshold = max_threshold\n",
    "                    )\n",
    "                    print(f\"[DONE] Saved reduced datasets for 'max' strategy ({max_threshold:.2f}).\\n\")\n",
    "\n",
    "    print(f\"[DONE] Completed processing for dataset: {dataset}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee07dc-47d7-4152-8d69-8a2f54f08877",
   "metadata": {},
   "source": [
    "## Retrain models on reduced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b2634-5761-47fb-8366-a93093a3655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory containing feature-reduced training datasets\n",
    "reduced_train_dir = '../data/reduced/train'\n",
    "\n",
    "# List all reduced training dataset files available in the directory\n",
    "reduced_datasets = os.listdir(reduced_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39902967-1031-4fc4-b0e7-49549d8c64ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrain models on reduced feature subsets for each dataset and model type\n",
    "for dataset in dataset_names:\n",
    "    # Convert dataset name to snake_case for consistency in file paths\n",
    "    set_name_snake = dataset.replace('-', '_')\n",
    "    print(f'\\n[INFO] Processing dataset: {dataset}')\n",
    "\n",
    "    # Loop through all model types\n",
    "    for model_type in model_types:\n",
    "        print(f'[INFO] Loading full {model_type} model for {dataset}...')\n",
    "        model = load_object(\n",
    "            os.path.join(models_dir, dataset, f'{set_name_snake}-{model_type}-full.pickle.xz')\n",
    "        )\n",
    "\n",
    "        # Select only the reduced datasets matching the current dataset and model type\n",
    "        filtered_reduced_datasets = [ds for ds in reduced_datasets if set_name_snake in ds and model_type in ds]\n",
    "        print(f'[INFO] Found {len(filtered_reduced_datasets)} reduced datasets for {model_type}.')\n",
    "\n",
    "        # Train the model on each reduced dataset\n",
    "        for reduced_dataset in filtered_reduced_datasets:\n",
    "            print(f'[INFO] Loading reduced dataset: {reduced_dataset}')\n",
    "            reduced_train = load_dataset(os.path.join(reduced_train_dir, reduced_dataset))\n",
    "            X_train = reduced_train.iloc[:, :-1]\n",
    "            y_train = reduced_train.iloc[:, -1]\n",
    "\n",
    "            # Extract feature selection type from filename\n",
    "            selection_type = reduced_dataset.split('.csv.gz')[0].split(f'{model_type}-')[-1]\n",
    "\n",
    "            # Define save path for the retrained model\n",
    "            save_path = os.path.join(models_dir, dataset, f'{set_name_snake}-{model_type}-{selection_type}')\n",
    "\n",
    "            # Retrain and save model without grid search using LZMA compression\n",
    "            print(f'[INFO] Retraining {model_type} with selection method: {selection_type}')\n",
    "            fit_model(\n",
    "                X_train = X_train,\n",
    "                y_train = y_train,\n",
    "                model_name = '',\n",
    "                model = clone(model),\n",
    "                grid_search = False,\n",
    "                save = True,\n",
    "                save_path = save_path,\n",
    "                compression = 'lzma'\n",
    "            )\n",
    "\n",
    "            print(f'[DONE] {model_type} ({selection_type}) retrained and saved for {dataset}.\\n')\n",
    "\n",
    "    print(f'[DONE] Completed retraining for all model types on {dataset}.\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
