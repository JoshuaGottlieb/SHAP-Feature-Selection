{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "y2eTEqmkpV6m"
      },
      "outputs": [],
      "source": [
        "!pip -q install shap xgboost\n",
        "\n",
        "import os, sys, math, json, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "\n",
        "RS = 42  # global seed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Column typing & encoders"
      ],
      "metadata": {
        "id": "RB3JopJB1RkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/sample_data/processed.cleveland.data\"\n",
        "\n",
        "if not os.path.exists(PATH):\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"Upload processed.cleveland.data\")\n",
        "        up = files.upload()\n",
        "        # File will land in /content with its original name\n",
        "        if \"processed.cleveland.data\" not in up:\n",
        "            raise RuntimeError(\"Please upload 'processed.cleveland.data'.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Couldn't find or upload 'processed.cleveland.data'\") from e"
      ],
      "metadata": {
        "id": "IGs1RwD62yfo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) Read & basic clean"
      ],
      "metadata": {
        "id": "QYA_C-Dw1lWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\n",
        "    'age','sex','cp','trestbps','chol','fbs','restecg',\n",
        "    'thalach','exang','oldpeak','slope','ca','thal','target'\n",
        "]\n",
        "df = pd.read_csv(PATH, header=None, names=cols, na_values=[\"?\"])\n",
        "# Convert to numeric (safeguard)\n",
        "for c in cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Binary target: 0 = no disease, 1 = disease (1..4 → 1)\n",
        "df['target'] = (df['target'] > 0).astype(int)\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(df.head())\n",
        "print(\"\\nMissing counts:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx800zeJqEZk",
        "outputId": "d2870d2c-16a2-4e99-c4e7-bb4cdd24753e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (303, 14)\n",
            "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
            "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
            "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
            "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
            "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
            "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
            "\n",
            "   slope   ca  thal  target  \n",
            "0    3.0  0.0   6.0       0  \n",
            "1    2.0  3.0   3.0       1  \n",
            "2    2.0  2.0   7.0       1  \n",
            "3    3.0  0.0   3.0       0  \n",
            "4    1.0  0.0   3.0       0  \n",
            "\n",
            "Missing counts:\n",
            " age         0\n",
            "sex         0\n",
            "cp          0\n",
            "trestbps    0\n",
            "chol        0\n",
            "fbs         0\n",
            "restecg     0\n",
            "thalach     0\n",
            "exang       0\n",
            "oldpeak     0\n",
            "slope       0\n",
            "ca          4\n",
            "thal        2\n",
            "target      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Define features & preprocessing"
      ],
      "metadata": {
        "id": "VhC86ZGC1pjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numeric vs categorical (these are coded integers but categorical by meaning)\n",
        "cat_cols = ['sex','cp','fbs','restecg','exang','slope','ca','thal']\n",
        "num_cols = [c for c in df.columns if c not in cat_cols + ['target']]\n",
        "\n",
        "numeric_pipe = SimpleImputer(strategy=\"median\")\n",
        "categorical_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_pipe, num_cols),\n",
        "        (\"cat\", categorical_pipe, cat_cols),\n",
        "    ],\n",
        "    sparse_threshold=1.0\n",
        ")\n",
        "\n",
        "X = df.drop(columns=[\"target\"])\n",
        "y = df[\"target\"]\n",
        "\n",
        "# Single stratified split (80% train / 20% test) — no secondary validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=RS\n",
        ")\n",
        "\n",
        "# Fit preprocess on TRAIN only; transform TRAIN and TEST\n",
        "preprocess.fit(X_train)\n",
        "Xtr_enc = preprocess.transform(X_train)\n",
        "Xte_enc = preprocess.transform(X_test)\n",
        "\n",
        "try:\n",
        "    feat_names = preprocess.get_feature_names_out()\n",
        "except:\n",
        "    # Fallback (rare in newer sklearn)\n",
        "    feat_names = [f\"f_{i}\" for i in range(Xtr_enc.shape[1])]"
      ],
      "metadata": {
        "id": "mNssg7FvqcR7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4) Baseline models (FULL features)"
      ],
      "metadata": {
        "id": "5v9l3Ei31tqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from sklearn.preprocessing import TargetEncoder  # sklearn >= 1.6\n",
        "    _SKLEARN_TE = True\n",
        "except Exception:\n",
        "    _SKLEARN_TE = False\n",
        "    try:\n",
        "        # Colab-friendly fallback\n",
        "        import sys, subprocess\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"category_encoders\"], check=False)\n",
        "        from category_encoders import TargetEncoder as CAT_TargetEncoder\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            \"TargetEncoder not available. Upgrade scikit-learn to >=1.6 OR allow installing category_encoders.\"\n",
        "        )\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV  # ← no StratifiedKFold\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score  # + average_precision_score\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n"
      ],
      "metadata": {
        "id": "sMpn_-TR8pIh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Column typing & encoders\n",
        "\n",
        "def n_unique_nonnull(series):\n",
        "    return series.dropna().nunique()\n",
        "\n",
        "card = {c: n_unique_nonnull(X_train[c]) for c in cat_cols}\n",
        "\n",
        "bin_cols   = [c for c,v in card.items() if v == 2]\n",
        "low_cols   = [c for c,v in card.items() if 3 <= v <= 10]\n",
        "high_cols  = [c for c,v in card.items() if v > 10]\n",
        "\n",
        "# Pipelines per block\n",
        "num_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "bin_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
        "])\n",
        "\n",
        "low_ohe_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "\n",
        "if _SKLEARN_TE:\n",
        "    te_estimator = TargetEncoder  # sklearn TargetEncoder\n",
        "else:\n",
        "    te_estimator = CAT_TargetEncoder  # category_encoders fallback\n",
        "\n",
        "high_te_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"te\", te_estimator())\n",
        "])\n",
        "\n",
        "# Build the ColumnTransformer; force dense output overall\n",
        "transformers = []\n",
        "if num_cols:\n",
        "    transformers.append((\"num\", num_pipe, num_cols))\n",
        "if bin_cols:\n",
        "    transformers.append((\"bin\", bin_pipe, bin_cols))\n",
        "if low_cols:\n",
        "    transformers.append((\"low\", low_ohe_pipe, low_cols))\n",
        "if high_cols:\n",
        "    transformers.append((\"high\", high_te_pipe, high_cols))\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=transformers,\n",
        "    remainder=\"drop\",\n",
        "    sparse_threshold=0.0   # force dense\n",
        ")"
      ],
      "metadata": {
        "id": "sMHsjXj38wjU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Define base models\n",
        "\n",
        "models = {\n",
        "    \"logreg\": LogisticRegression(random_state=RS, solver=\"saga\", max_iter=3000),\n",
        "    \"dtree\":  DecisionTreeClassifier(random_state=RS),\n",
        "    \"rf\":     RandomForestClassifier(random_state=RS, n_jobs=-1),\n",
        "    \"xgb\":    XGBClassifier(\n",
        "                 random_state=RS,\n",
        "                 tree_method=\"hist\",\n",
        "                 # silent defaults; other params tuned in grid\n",
        "                 n_jobs=-1\n",
        "             ),\n",
        "    \"svc\":    SVC(random_state=RS, probability=True)\n",
        "}"
      ],
      "metadata": {
        "id": "uyt5JhqS82jF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Grids\n",
        "\n",
        "logreg_grid = {\n",
        "    'logreg__penalty': ['l2', 'l1'],\n",
        "    'logreg__C': [10**x for x in range(-2, 3)]\n",
        "}\n",
        "\n",
        "dtree_grid = {\n",
        "    'dtree__max_depth': [None] + [x for x in range(3, 11, 2)],\n",
        "    'dtree__min_samples_split': [x for x in range(2, 14, 4)],\n",
        "    'dtree__min_samples_leaf': [1, 10, 20]\n",
        "}\n",
        "\n",
        "rf_grid = {\n",
        "    'rf__n_estimators': [100, 200, 400, 600],\n",
        "    'rf__max_depth': [None] + [x for x in range(3, 11, 2)],\n",
        "    'rf__min_samples_split': [x for x in range(2, 14, 4)],\n",
        "    'rf__min_samples_leaf': [1, 10, 20]\n",
        "}\n",
        "\n",
        "xgb_grid = {\n",
        "    'xgb__n_estimators': [100, 200, 400],\n",
        "    'xgb__max_depth': [3, 4, 5],\n",
        "    'xgb__min_child_weight': [1, 3, 5],\n",
        "    'xgb__subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'xgb__colsample_bytree': [0.7, 0.8, 0.9, 1.0]  # NOTE: correct name is \"colsample_bytree\"\n",
        "}\n",
        "\n",
        "svc_grid = {\n",
        "    'svc__C': [10**x for x in range(-2, 3)],\n",
        "    'svc__gamma': [10**x for x in range(-2, 3)]\n",
        "}\n",
        "\n",
        "grids = {\n",
        "    \"logreg\": logreg_grid,\n",
        "    \"dtree\":  dtree_grid,\n",
        "    \"rf\":     rf_grid,\n",
        "    \"xgb\":    xgb_grid,\n",
        "    \"svc\":    svc_grid\n",
        "}"
      ],
      "metadata": {
        "id": "0uXlJAuJ86M-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Pipelines & Search\n",
        "from sklearn.model_selection import GridSearchCV  # ← removed StratifiedKFold, RandomizedSearchCV\n",
        "import joblib\n",
        "import time\n",
        "\n",
        "# 1) CV & scoring per feedback\n",
        "cv = 5                                  # ← use integer cv=5\n",
        "scoring = \"average_precision\"           # ← AP instead of ROC AUC\n",
        "\n",
        "# 2) Optional: cache preprocessing (BIG speedup)\n",
        "CACHE_DIR = \"/content/cache\"\n",
        "memory = joblib.Memory(CACHE_DIR, verbose=0)\n",
        "\n",
        "# 3) XGB stays CPU-only (no gpu_hist switch)\n",
        "#    (base model already has tree_method=\"hist\")\n",
        "\n",
        "dataset_name = \"cleveland-heart\"\n",
        "results = {}\n",
        "\n",
        "# 4) Grid search spaces ONLY (no randomized)\n",
        "logreg_grid = {\n",
        "    'logreg__penalty': ['l2', 'l1'],\n",
        "    'logreg__C': [0.01, 0.1, 1, 10, 100],\n",
        "}\n",
        "\n",
        "dtree_grid = {\n",
        "    'dtree__max_depth': [None, 3, 5, 7, 9],\n",
        "    'dtree__min_samples_split': [2, 6, 10],\n",
        "    'dtree__min_samples_leaf': [1, 5, 10],\n",
        "}\n",
        "\n",
        "rf_grid = {                              # ← converted from randomized to grid\n",
        "    'rf__n_estimators': [120, 180, 240, 300],\n",
        "    'rf__max_depth': [None, 5, 7, 9],\n",
        "    'rf__min_samples_split': [2, 6, 10],\n",
        "    'rf__min_samples_leaf': [1, 3, 5],\n",
        "}\n",
        "\n",
        "xgb_grid = {                             # ← converted from randomized to grid\n",
        "    'xgb__n_estimators': [120, 180, 240],\n",
        "    'xgb__max_depth': [3, 4, 5],\n",
        "    'xgb__min_child_weight': [1, 3, 5],\n",
        "    'xgb__subsample': [0.8, 1.0],\n",
        "    'xgb__colsample_bytree': [0.8, 1.0],\n",
        "}\n",
        "\n",
        "svc_grid = {                             # ← converted from randomized to grid\n",
        "    'svc__C': [0.1, 1, 10],\n",
        "    'svc__gamma': ['scale', 0.1, 0.01],\n",
        "}\n",
        "\n",
        "search_plan = {\n",
        "    \"logreg\": logreg_grid,\n",
        "    \"dtree\":  dtree_grid,\n",
        "    \"rf\":     rf_grid,\n",
        "    \"xgb\":    xgb_grid,\n",
        "    \"svc\":    svc_grid,\n",
        "}\n",
        "\n",
        "print(\"GRID MODE (cv=5, scoring=average_precision): starting searches...\\n\")\n",
        "\n",
        "for name, estimator in models.items():\n",
        "    # Pipeline with caching\n",
        "    pipe = Pipeline(steps=[(\"prep\", preprocess), (name, estimator)], memory=memory)\n",
        "\n",
        "    searcher = GridSearchCV(\n",
        "        pipe, search_plan[name], scoring=scoring, cv=cv, n_jobs=-1, refit=True, verbose=0\n",
        "    )\n",
        "\n",
        "    t0 = time.time()\n",
        "    searcher.fit(X_train, y_train)  # TRAIN ONLY\n",
        "    fit_secs = time.time() - t0\n",
        "\n",
        "    # Evaluate on TEST\n",
        "    proba = searcher.predict_proba(X_test)[:, 1]\n",
        "    pred  = (proba >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, pred)\n",
        "    f1  = f1_score(y_test, pred)\n",
        "    ap  = average_precision_score(y_test, proba)      # ← report AP\n",
        "    auc = roc_auc_score(y_test, proba)                # (optional: still report AUC)\n",
        "\n",
        "    results[name] = {\n",
        "        \"best_params\": searcher.best_params_,\n",
        "        \"best_score_cv\": searcher.best_score_,        # this is AP under the new scoring\n",
        "        \"test_acc\": acc,\n",
        "        \"test_f1\": f1,\n",
        "        \"test_ap\": ap,                                # ← store AP\n",
        "        \"test_auc\": auc,\n",
        "        \"secs\": fit_secs,\n",
        "        \"model\": searcher\n",
        "    }\n",
        "\n",
        "    # Save each fitted searcher (pipeline included)\n",
        "    fname = f\"{dataset_name}-{name}-full.pickle\"\n",
        "    with open(fname, \"wb\") as f:\n",
        "        pickle.dump(searcher, f)\n",
        "\n",
        "    print(f\"{name.upper():5s} done in {fit_secs/60:.1f} min | \"\n",
        "          f\"AP={ap:.3f} AUC={auc:.3f} ACC={acc:.3f} F1={f1:.3f}\")\n",
        "\n",
        "print(\"\\n=== GRID MODE: Test Metrics (scoring=average_precision) ===\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k.upper():5s} | AP={v['test_ap']:.3f}  AUC={v['test_auc']:.3f}  \"\n",
        "          f\"ACC={v['test_acc']:.3f}  F1={v['test_f1']:.3f}  (CV best AP={v['best_score_cv']:.3f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UDr23Q18-8q",
        "outputId": "af998fbc-e219-41e7-89c6-3864ab3cbfe9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRID MODE (cv=5, scoring=average_precision): starting searches...\n",
            "\n",
            "LOGREG done in 0.2 min | AP=0.949 AUC=0.958 ACC=0.885 F1=0.877\n",
            "DTREE done in 0.1 min | AP=0.794 AUC=0.867 ACC=0.820 F1=0.814\n",
            "RF    done in 5.6 min | AP=0.941 AUC=0.951 ACC=0.852 F1=0.847\n",
            "XGB   done in 0.5 min | AP=0.923 AUC=0.950 ACC=0.885 F1=0.885\n",
            "SVC   done in 0.0 min | AP=0.749 AUC=0.795 ACC=0.754 F1=0.717\n",
            "\n",
            "=== GRID MODE: Test Metrics (scoring=average_precision) ===\n",
            "LOGREG | AP=0.949  AUC=0.958  ACC=0.885  F1=0.877  (CV best AP=0.856)\n",
            "DTREE | AP=0.794  AUC=0.867  ACC=0.820  F1=0.814  (CV best AP=0.804)\n",
            "RF    | AP=0.941  AUC=0.951  ACC=0.852  F1=0.847  (CV best AP=0.901)\n",
            "XGB   | AP=0.923  AUC=0.950  ACC=0.885  F1=0.885  (CV best AP=0.870)\n",
            "SVC   | AP=0.749  AUC=0.795  ACC=0.754  F1=0.717  (CV best AP=0.733)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Report\n",
        "\n",
        "print(\"\\n=== Test Metrics (held-out TEST) ===\")\n",
        "for k, v in results.items():\n",
        "    print(\n",
        "        f\"{k.upper():5s} | AP={v['test_ap']:.3f}  AUC={v['test_auc']:.3f}  \"\n",
        "        f\"ACC={v['test_acc']:.3f}  F1={v['test_f1']:.3f}  (CV best AP={v['best_score_cv']:.3f})\"\n",
        "    )\n",
        "\n",
        "print(\"\\nBest params per model:\")\n",
        "for k, v in results.items():\n",
        "    print(k, \"→\", v[\"best_params\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYbfJJh8q-ko",
        "outputId": "b97bfb7f-d09c-4d53-bc0f-fa93e293d4ae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Metrics (held-out TEST) ===\n",
            "LOGREG | AP=0.949  AUC=0.958  ACC=0.885  F1=0.877  (CV best AP=0.856)\n",
            "DTREE | AP=0.794  AUC=0.867  ACC=0.820  F1=0.814  (CV best AP=0.804)\n",
            "RF    | AP=0.941  AUC=0.951  ACC=0.852  F1=0.847  (CV best AP=0.901)\n",
            "XGB   | AP=0.923  AUC=0.950  ACC=0.885  F1=0.885  (CV best AP=0.870)\n",
            "SVC   | AP=0.749  AUC=0.795  ACC=0.754  F1=0.717  (CV best AP=0.733)\n",
            "\n",
            "Best params per model:\n",
            "logreg → {'logreg__C': 10, 'logreg__penalty': 'l1'}\n",
            "dtree → {'dtree__max_depth': None, 'dtree__min_samples_leaf': 10, 'dtree__min_samples_split': 2}\n",
            "rf → {'rf__max_depth': None, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 120}\n",
            "xgb → {'xgb__colsample_bytree': 0.8, 'xgb__max_depth': 3, 'xgb__min_child_weight': 5, 'xgb__n_estimators': 180, 'xgb__subsample': 0.8}\n",
            "svc → {'svc__C': 10, 'svc__gamma': 'scale'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Xx8gk_p-OOq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}