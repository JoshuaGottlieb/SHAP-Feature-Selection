{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c528b638-3fca-400a-abcd-0f6127cb95e1",
   "metadata": {},
   "source": [
    "## Load libraries and set up directory paths and iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccf291-05fc-4c9d-a0fc-f476227bbf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from modules.utils import load_object, load_dataset, save_object\n",
    "from modules.explanations import sample_shap_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544c7d6-a567-44c8-aa83-e3ae89f8c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory paths for models, datasets, and SHAP explanations\n",
    "models_dir = '../models'\n",
    "datasets_dir = '../data/processed/train'\n",
    "explanations_dir = '../shap-explanations'\n",
    "\n",
    "# Define iterables for looping over datasets and model types\n",
    "dataset_names = sorted(os.listdir(models_dir))\n",
    "model_types = ['logreg', 'dt', 'rf', 'xgb', 'svc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b94698-e9a6-4c0e-91ce-8d426cf9dd57",
   "metadata": {},
   "source": [
    "## Compute global SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a599305c-82a9-45fb-9cbc-6b64cbb3e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Takes many hours to run (>12 hours for certain models) and was performed per-dataset on Google Colab\n",
    "\n",
    "# Compute SHAP explanations for each dataset and model\n",
    "for name in dataset_names:\n",
    "    print(f'Computing SHAP explanations for {name}')\n",
    "    \n",
    "    # Directory to save SHAP explanations for the current dataset\n",
    "    explanations_subdir = os.path.join(explanations_dir, name)\n",
    "\n",
    "    # Create directory if it does not exist\n",
    "    if not os.path.exists(explanations_subdir):\n",
    "        print(f'Creating directory for SHAP explanations at {explanations_subdir}')\n",
    "        os.makedirs(explanations_subdir)\n",
    "\n",
    "    # Load the training dataset\n",
    "    training_set_path = os.path.join(datasets_dir, name.replace('-', '_') + '-train.csv.gz')\n",
    "    print(f'Loading dataset at {training_set_path}')\n",
    "    training_set = load_dataset(training_set_path)\n",
    "\n",
    "    # Compute SHAP explanations for each model type\n",
    "    for type_ in model_types:\n",
    "        # Load the trained model\n",
    "        model_path = os.path.join(models_dir, name, name.replace('-', '_') + f'-{type_}-full.pickle.xz')\n",
    "        print(f'Loading model at {model_path}')\n",
    "        model = load_object(model_path)\n",
    "        \n",
    "        # Path to save the computed SHAP explanation\n",
    "        explanation_path = os.path.join(\n",
    "            explanations_subdir, name.replace('-', '_') + f\"-{type_}-sampling-global\"\n",
    "        )\n",
    "        \n",
    "        # Compute SHAP explanations\n",
    "        shap_explanation = sample_shap_explanations(\n",
    "            model_eval = model.predict,\n",
    "            data = training_set,\n",
    "            verbose = True\n",
    "        )\n",
    "\n",
    "        # Save SHAP explanation with LZMA compression\n",
    "        save_object(shap_explanation, explanation_path, compression = 'lzma')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
