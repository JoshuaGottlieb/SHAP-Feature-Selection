{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7157ce94-2314-423e-97f0-9edfebdb0087",
   "metadata": {},
   "source": [
    "## Load data, define models and search grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80155282-5752-496e-9870-829593822fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBClassifier\n",
    "from modules.utils import load_object, load_dataset\n",
    "from modules.training import fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da43104-c2e1-4ee0-92ee-8c874c8ce8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset paths and names\n",
    "training_dir = '../data/processed/train'\n",
    "dataset_paths = os.listdir(training_dir)\n",
    "dataset_names = [path.split('-')[0] for path in dataset_paths]\n",
    "dataset_name_paths = list(zip(dataset_names, dataset_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba401ff-46f4-4713-ba5d-05f5b5f70547",
   "metadata": {},
   "source": [
    "## Define classifiers and grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f569b78-74d5-479c-a2f5-77ddc6552fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base classifiers with fixed random seeds for reproducibility\n",
    "# Using saga solver in LogisticRegression for L1 and L2 penalty compatability\n",
    "base_logreg = LogisticRegression(random_state = 42, solver = 'saga', max_iter = 3000)\n",
    "base_dtree = DecisionTreeClassifier(random_state = 42)\n",
    "base_rf = RandomForestClassifier(random_state = 42)\n",
    "base_xgb = XGBClassifier(random_state = 42)\n",
    "base_svc = SVC(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b2047-bafe-46b9-86ae-4dd9ec0c359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids for each base classifier\n",
    "\n",
    "# Logistic Regression: regularization type and strength\n",
    "logreg_grid = {\n",
    "    'logreg__penalty': ['l2', 'l1'],\n",
    "    'logreg__C': [10**x for x in range(-2, 3)]\n",
    "}\n",
    "\n",
    "# Decision Tree: depth and split constraints\n",
    "dtree_grid = {\n",
    "    'dt__max_depth': [None] + [x for x in range(3, 11, 2)],\n",
    "    'dt__min_samples_split': [x for x in range(2, 14, 4)],\n",
    "    'dt__min_samples_leaf': [1, 10, 20]\n",
    "}\n",
    "\n",
    "# Random Forest: number of trees, depth, and split constraints\n",
    "rf_grid = {\n",
    "    'rf__n_estimators': [100, 200, 400, 600],\n",
    "    'rf__max_depth': [None] + [x for x in range(3, 11, 2)],\n",
    "    'rf__min_samples_split': [x for x in range(2, 14, 4)],\n",
    "    'rf__min_samples_leaf': [1, 10, 20]\n",
    "}\n",
    "\n",
    "# XGBoost: tree count, depth, sampling, and regularization parameters\n",
    "xgb_grid = {\n",
    "    'xgb__n_estimators': [100, 200, 400],\n",
    "    'xgb__max_depth': [3, 4, 5],\n",
    "    'xgb__min_child_weight': [1, 3, 5],\n",
    "    'xgb__subsample': [0.7, 0.8, 0.9, 1],\n",
    "    'xgb__colsample_bytree': [0.7, 0.8, 0.9, 1]\n",
    "}\n",
    "\n",
    "# SVC: regularization strength and kernel coefficient\n",
    "svc_grid = {\n",
    "    'svc__C': [10**x for x in range(-2, 3)],\n",
    "    'svc__gamma': [10**x for x in range(-2, 3)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec963d-26bd-4a48-b97b-f0aaa85686fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap model names, base classifiers, and their corresponding grids\n",
    "\n",
    "# Model identifiers\n",
    "models = ['logreg', 'dt', 'rf', 'xgb', 'svc']\n",
    "\n",
    "# Base classifier instances (aligned with model names)\n",
    "base_estimators = [base_logreg, base_dtree, base_rf, base_xgb, base_svc]\n",
    "\n",
    "# Hyperparameter grids for each model\n",
    "grids = [logreg_grid, dtree_grid, rf_grid, xgb_grid, svc_grid]\n",
    "\n",
    "# Combine model name, estimator, and grid into a single iterable\n",
    "model_grids = list(zip(models, base_estimators, grids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58763bcf-7d1c-46fa-9c65-dd66b52b4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base classifiers optimized for larger datasets (faster or more scalable variants)\n",
    "base_logreg_big = LogisticRegression(random_state = 42, solver = 'saga', max_iter = 3000)\n",
    "base_dtree_big = DecisionTreeClassifier(random_state = 42)\n",
    "base_rf_big = RandomForestClassifier(random_state = 42)\n",
    "base_xgb_big = XGBClassifier(random_state = 42)\n",
    "\n",
    "# Linear SVC instead of SVC with rbf kernel for larger datasets, as rbf kernel scales poorly with dataset size\n",
    "base_svc_big = LinearSVC(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbdd56-5aa8-4044-8863-61f4735a1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids for larger datasets (reduced search space for efficiency)\n",
    "\n",
    "# Logistic Regression: regularization type and strength\n",
    "logreg_grid_big = {\n",
    "    'logreg__penalty': ['l2', 'l1'],\n",
    "    'logreg__C': [10**x for x in range(-2, 3)]\n",
    "}\n",
    "\n",
    "# Decision Tree: depth and split constraints\n",
    "dtree_grid_big = {\n",
    "    'dt__max_depth': [None] + [x for x in range(3, 11, 2)],\n",
    "    'dt__min_samples_split': [x for x in range(2, 14, 4)],\n",
    "    'dt__min_samples_leaf': [1, 10, 20]\n",
    "}\n",
    "\n",
    "# Random Forest: smaller grid for faster tuning\n",
    "rf_grid_big = {\n",
    "    'rf__n_estimators': [400],\n",
    "    'rf__max_depth': [None] + [x for x in range(3, 7, 2)],\n",
    "    'rf__min_samples_split': [x for x in range(2, 10, 4)]\n",
    "}\n",
    "\n",
    "# XGBoost: reduced grid for speed, maintaining core parameters\n",
    "xgb_grid_big = {\n",
    "    'xgb__n_estimators': [400],\n",
    "    'xgb__max_depth': [3, 4, 5],\n",
    "    'xgb__min_child_weight': [1, 3, 5],\n",
    "    'xgb__subsample': [0.8, 1],\n",
    "    'xgb__colsample_bytree': [0.8, 1]\n",
    "}\n",
    "\n",
    "# Linear SVC: regularization strength only (simplified for speed)\n",
    "svc_grid_big = {\n",
    "    'svc__C': [10**x for x in range(-2, 6)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562fff8-dc97-4c9c-9f7c-7e04e7f8479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define big datasets and wrap corresponding base estimators and grids\n",
    "\n",
    "# Dataset names used for large-scale experiments\n",
    "big_datasets = [\n",
    "    'kaggle-credit-card-fraud',\n",
    "    'kaggle-patient-survival',\n",
    "    'secondary-mushroom',\n",
    "    'uci-android-permissions',\n",
    "    'uci-phishing-url'\n",
    "]\n",
    "\n",
    "# Base classifiers optimized for big datasets\n",
    "base_estimators_big = [base_logreg_big, base_dtree_big, base_rf_big, base_xgb_big, base_svc_big]\n",
    "\n",
    "# Corresponding hyperparameter grids for each model\n",
    "grids_big = [logreg_grid_big, dtree_grid_big, rf_grid_big, xgb_grid_big, svc_grid_big]\n",
    "\n",
    "# Combine model name, estimator, and grid into one iterable for big dataset runs\n",
    "model_grids_big = list(zip(models, base_estimators_big, grids_big))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee37186-c1bd-4e45-bd2d-950a7bf706cc",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc388ccf-bd3a-4eb5-afeb-5c208031694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and hyperparameter-tune models for each dataset\n",
    "for name, dataset_path in dataset_name_paths:\n",
    "    print(f'Loading {name} dataset.')\n",
    "    \n",
    "    # Load training data and split into features and target\n",
    "    train_dataset = load_dataset(os.path.join(training_dir, dataset_path))\n",
    "    X_train = train_dataset.iloc[:, :-1]\n",
    "    y_train = train_dataset.iloc[:, -1]\n",
    "    \n",
    "    # Define directory for saving trained models\n",
    "    model_dir = os.path.join('../models', '-'.join(name.split('_')))\n",
    "\n",
    "    # Use smaller grids and fewer folds for large datasets\n",
    "    if name in big_datasets:\n",
    "        for model_name, base_estimator, grid in model_grids_big:\n",
    "            print(f'Training {model_name} on {name}.')\n",
    "            save_path = os.path.join(model_dir, f'{name}-{model_name}-full')\n",
    "    \n",
    "            # Fit model with 3-fold cross-validation and save results\n",
    "            fit_model(\n",
    "                X_train = X_train,\n",
    "                y_train = y_train,\n",
    "                model_name = model_name,\n",
    "                model = clone(base_estimator),\n",
    "                grid_search = True,\n",
    "                param_grid = grid,\n",
    "                cv = 3,\n",
    "                save = True,\n",
    "                save_path = save_path\n",
    "            )\n",
    "\n",
    "    # Use full grids and more folds for regular-sized datasets\n",
    "    else:\n",
    "        for model_name, base_estimator, grid in model_grids:\n",
    "            print(f'Training {model_name} on {name}.')\n",
    "            save_path = os.path.join(model_dir, f'{name}-{model_name}-full')\n",
    "    \n",
    "            # Fit model with 5-fold cross-validation and save results\n",
    "            fit_model(\n",
    "                X_train = X_train,\n",
    "                y_train = y_train,\n",
    "                model_name = model_name,\n",
    "                model = clone(base_estimator),\n",
    "                grid_search = True,\n",
    "                param_grid = grid,\n",
    "                cv = 5,\n",
    "                save = True,\n",
    "                save_path = save_path\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
