{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic86OZ76Bptd",
        "outputId": "386632a7-5115-4635-fdef-0733bbd8721f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (80, 22) | Test shape: (187, 22)\n",
            "Train class counts: [40 40]\n",
            "Test  class counts: [ 15 172]\n",
            "\n",
            "Searching: LOGREG\n",
            "LOGREG done in 0.00 min | AUC=0.841 ACC=0.754 F1=0.851\n",
            "\n",
            "Searching: DTREE\n",
            "DTREE done in 0.01 min | AUC=0.746 ACC=0.706 F1=0.814\n",
            "\n",
            "Searching: RF\n",
            "RF done in 0.94 min | AUC=0.827 ACC=0.797 F1=0.879\n",
            "\n",
            "Searching: XGB\n",
            "XGB done in 0.06 min | AUC=0.822 ACC=0.722 F1=0.827\n",
            "\n",
            "Searching: SVC\n",
            "SVC done in 0.00 min | AUC=0.817 ACC=0.856 F1=0.918\n",
            "\n",
            "=== Test Metrics (held-out TEST) ===\n",
            "LOGREG | AUC=0.841  ACC=0.754  F1=0.851  (CV best=0.792)\n",
            "DTREE | AUC=0.746  ACC=0.706  F1=0.814  (CV best=0.748)\n",
            "RF    | AUC=0.827  ACC=0.797  F1=0.879  (CV best=0.778)\n",
            "XGB   | AUC=0.822  ACC=0.722  F1=0.827  (CV best=nan)\n",
            "SVC   | AUC=0.817  ACC=0.856  F1=0.918  (CV best=0.810)\n",
            "\n",
            "Best params per model:\n",
            "logreg → {'logreg__C': 0.01, 'logreg__penalty': 'l2'}\n",
            "dtree → {'dtree__max_depth': 3, 'dtree__min_samples_leaf': 1, 'dtree__min_samples_split': 2}\n",
            "rf → {'rf__max_depth': 4, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 200}\n",
            "xgb → {'xgb__gamma': 0, 'xgb__max_depth': 3, 'xgb__min_child_weight': 1, 'xgb__reg_lambda': 1.0}\n",
            "svc → {'svc__C': 0.1, 'svc__kernel': 'rbf'}\n",
            "\n",
            "Total elapsed: 1.02 min\n",
            "\n",
            "=== Detailed classification report: LOGREG ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.196     0.667     0.303        15\n",
            "           1      0.963     0.762     0.851       172\n",
            "\n",
            "    accuracy                          0.754       187\n",
            "   macro avg      0.580     0.714     0.577       187\n",
            "weighted avg      0.902     0.754     0.807       187\n",
            "\n",
            "Confusion matrix:\n",
            " [[ 10   5]\n",
            " [ 41 131]]\n",
            "\n",
            "Artifacts saved to: /content/spect_outputs\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# SPECT Heart - Full Colab Script\n",
        "# Compatible: scikit-learn 1.4.2, xgboost 2.1.1\n",
        "# ===============================\n",
        "\n",
        "import os, time, json, warnings, pathlib\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, accuracy_score, f1_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.utils import Bunch\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# --------------- Settings ---------------\n",
        "RS = 42\n",
        "CV_N_SPLITS = 3              # change to 5 if you used 5-fold in Oct13\n",
        "SCORING = \"roc_auc\"          # change to \"average_precision\" if needed\n",
        "TRAIN_PATH = \"/content/sample_data/SPECT.train\"\n",
        "TEST_PATH  = \"/content/sample_data/SPECT.test\"\n",
        "\n",
        "assert os.path.exists(TRAIN_PATH), f\"File not found: {TRAIN_PATH}\"\n",
        "assert os.path.exists(TEST_PATH),  f\"File not found: {TEST_PATH}\"\n",
        "\n",
        "# --------------- Data Loading ---------------\n",
        "def load_spect(path: str) -> Bunch:\n",
        "    # UCI SPECT: first column is label, rest are binary features\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    n_features = df.shape[1] - 1\n",
        "    df.columns = [\"target\"] + [f\"x{i+1}\" for i in range(n_features)]\n",
        "    # coerce numeric + drop NA\n",
        "    for c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df = df.dropna().astype(int)\n",
        "    X = df.drop(columns=[\"target\"]).values\n",
        "    y = df[\"target\"].values\n",
        "    return Bunch(frame=df, X=X, y=y, feature_names=df.columns[1:])\n",
        "\n",
        "train = load_spect(TRAIN_PATH)\n",
        "test  = load_spect(TEST_PATH)\n",
        "\n",
        "print(\"Train shape:\", train.X.shape, \"| Test shape:\", test.X.shape)\n",
        "print(\"Train class counts:\", np.bincount(train.y))\n",
        "print(\"Test  class counts:\", np.bincount(test.y))\n",
        "\n",
        "# If your positive class is flipped vs. Oct13, uncomment:\n",
        "# train.y = 1 - train.y\n",
        "# test.y  = 1 - test.y\n",
        "\n",
        "# --------------- Pipelines ---------------\n",
        "cv = StratifiedKFold(n_splits=CV_N_SPLITS, shuffle=True, random_state=RS)\n",
        "\n",
        "pipelines = {\n",
        "    \"logreg\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"logreg\", LogisticRegression(max_iter=1000, random_state=RS))\n",
        "    ]),\n",
        "    \"dtree\": Pipeline([\n",
        "        (\"dtree\", DecisionTreeClassifier(random_state=RS))\n",
        "    ]),\n",
        "    \"rf\": Pipeline([\n",
        "        (\"rf\", RandomForestClassifier(random_state=RS, n_jobs=-1))\n",
        "    ]),\n",
        "    \"xgb\": Pipeline([\n",
        "        (\"xgb\", XGBClassifier(\n",
        "            random_state=RS,\n",
        "            n_estimators=300,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.9,\n",
        "            colsample_bytree=0.9,\n",
        "            tree_method=\"hist\",   # CPU-friendly and portable\n",
        "            n_jobs=-1,\n",
        "            eval_metric=\"logloss\"\n",
        "        ))\n",
        "    ]),\n",
        "    \"svc\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svc\", SVC(probability=True, random_state=RS))\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# --------------- Parameter Grids ---------------\n",
        "param_grids = {\n",
        "    \"logreg\": {\n",
        "        \"logreg__penalty\": [\"l2\"],\n",
        "        \"logreg__C\": [0.01, 0.1, 1.0, 10.0]\n",
        "    },\n",
        "    \"dtree\": {\n",
        "        \"dtree__max_depth\": [None, 3, 4, 5],\n",
        "        \"dtree__min_samples_split\": [2, 5, 10],\n",
        "        \"dtree__min_samples_leaf\": [1, 5, 10]\n",
        "    },\n",
        "    \"rf\": {\n",
        "        \"rf__n_estimators\": [200, 400],\n",
        "        \"rf__max_depth\": [None, 4, 6],\n",
        "        \"rf__min_samples_split\": [2, 5, 10],\n",
        "        \"rf__min_samples_leaf\": [1, 5, 10]\n",
        "    },\n",
        "    \"xgb\": {\n",
        "        \"xgb__max_depth\": [3, 4, 5],\n",
        "        \"xgb__min_child_weight\": [1, 2, 4],\n",
        "        \"xgb__gamma\": [0, 0.5],\n",
        "        \"xgb__reg_lambda\": [1.0, 2.0, 5.0]\n",
        "    },\n",
        "    \"svc\": {\n",
        "        \"svc__C\": [0.1, 1.0, 10.0],\n",
        "        \"svc__kernel\": [\"rbf\", \"linear\"]\n",
        "    },\n",
        "}\n",
        "\n",
        "# --------------- Helpers (robust to API quirks) ---------------\n",
        "def proba_from_model(best_model, X, model_key):\n",
        "    \"\"\"\n",
        "    Robustly get probabilities for positive class.\n",
        "    Safely handles XGB inside a Pipeline.\n",
        "    \"\"\"\n",
        "    if model_key == \"xgb\":\n",
        "        est = getattr(best_model, \"named_steps\", {}).get(\"xgb\", best_model)\n",
        "        # XGBClassifier supports predict_proba\n",
        "        proba = est.predict_proba(X)\n",
        "        return proba[:, 1] if proba.ndim == 2 and proba.shape[1] > 1 else proba.ravel()\n",
        "\n",
        "    if hasattr(best_model, \"predict_proba\"):\n",
        "        proba = best_model.predict_proba(X)\n",
        "        return proba[:, 1] if proba.ndim == 2 and proba.shape[1] > 1 else proba.ravel()\n",
        "\n",
        "    if hasattr(best_model, \"decision_function\"):\n",
        "        scores = best_model.decision_function(X)\n",
        "        ranks = pd.Series(scores).rank(method=\"average\").values\n",
        "        return (ranks - ranks.min()) / (ranks.max() - ranks.min() + 1e-9)\n",
        "\n",
        "    return best_model.predict(X)\n",
        "\n",
        "\n",
        "def predict_from_model(best_model, X, model_key):\n",
        "    \"\"\"\n",
        "    Robustly get hard class predictions.\n",
        "    Safely handles XGB inside a Pipeline.\n",
        "    \"\"\"\n",
        "    if model_key == \"xgb\":\n",
        "        est = getattr(best_model, \"named_steps\", {}).get(\"xgb\", best_model)\n",
        "        return est.predict(X)\n",
        "    return best_model.predict(X)\n",
        "\n",
        "# --------------- Train + Evaluate ---------------\n",
        "results = {}\n",
        "start_all = time.time()\n",
        "\n",
        "for name, pipe in pipelines.items():\n",
        "    print(f\"\\nSearching: {name.upper()}\")\n",
        "    t0 = time.time()\n",
        "    gs = GridSearchCV(\n",
        "        estimator=pipe,\n",
        "        param_grid=param_grids[name],\n",
        "        scoring=SCORING,\n",
        "        cv=cv,\n",
        "        n_jobs=-1,\n",
        "        verbose=0,\n",
        "        refit=True\n",
        "    )\n",
        "    gs.fit(train.X, train.y)\n",
        "    dur_min = (time.time() - t0) / 60.0\n",
        "\n",
        "    best_model = gs.best_estimator_\n",
        "\n",
        "    # robust proba/preds (handles XGB inside Pipeline)\n",
        "    y_proba = proba_from_model(best_model, test.X, name)\n",
        "    y_pred  = predict_from_model(best_model, test.X, name)\n",
        "\n",
        "    # Metrics\n",
        "    test_auc = roc_auc_score(test.y, y_proba)\n",
        "    test_acc = accuracy_score(test.y, y_pred)\n",
        "    test_f1  = f1_score(test.y, y_pred, zero_division=0)\n",
        "\n",
        "    print(f\"{name.upper()} done in {dur_min:.2f} min | \"\n",
        "          f\"AUC={test_auc:.3f} ACC={test_acc:.3f} F1={test_f1:.3f}\")\n",
        "\n",
        "    results[name] = {\n",
        "        \"best_score_cv\": gs.best_score_,\n",
        "        \"best_params\": gs.best_params_,\n",
        "        \"test_auc\": float(test_auc),\n",
        "        \"test_acc\": float(test_acc),\n",
        "        \"test_f1\":  float(test_f1),\n",
        "        \"fit_time_min\": dur_min\n",
        "    }\n",
        "\n",
        "elapsed_all = (time.time() - start_all) / 60.0\n",
        "\n",
        "# --------------- Summary ---------------\n",
        "print(\"\\n=== Test Metrics (held-out TEST) ===\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k.upper():5s} | AUC={v['test_auc']:.3f}  \"\n",
        "          f\"ACC={v['test_acc']:.3f}  F1={v['test_f1']:.3f}  \"\n",
        "          f\"(CV best={v['best_score_cv']:.3f})\")\n",
        "\n",
        "print(\"\\nBest params per model:\")\n",
        "for k, v in results.items():\n",
        "    print(k, \"→\", v[\"best_params\"])\n",
        "\n",
        "print(f\"\\nTotal elapsed: {elapsed_all:.2f} min\")\n",
        "\n",
        "# --------------- Detailed report for top model ---------------\n",
        "top = max(results.items(), key=lambda kv: kv[1][\"test_auc\"])[0]\n",
        "print(f\"\\n=== Detailed classification report: {top.upper()} ===\")\n",
        "\n",
        "gs_top = GridSearchCV(\n",
        "    estimator=pipelines[top],\n",
        "    param_grid=param_grids[top],\n",
        "    scoring=SCORING,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    refit=True,\n",
        "    verbose=0\n",
        ")\n",
        "gs_top.fit(train.X, train.y)\n",
        "best_top = gs_top.best_estimator_\n",
        "\n",
        "y_pred_top  = predict_from_model(best_top, test.X, top)\n",
        "print(classification_report(test.y, y_pred_top, digits=3))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(test.y, y_pred_top))\n",
        "\n",
        "# --------------- Save artifacts (optional) ---------------\n",
        "OUTDIR = pathlib.Path(\"/content/spect_outputs\")\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(OUTDIR / \"results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# Save best estimators for each model\n",
        "import joblib\n",
        "for name, pipe in pipelines.items():\n",
        "    gs = GridSearchCV(\n",
        "        estimator=pipe,\n",
        "        param_grid=param_grids[name],\n",
        "        scoring=SCORING,\n",
        "        cv=cv,\n",
        "        n_jobs=-1,\n",
        "        refit=True,\n",
        "        verbose=0\n",
        "    )\n",
        "    gs.fit(train.X, train.y)\n",
        "    joblib.dump(gs.best_estimator_, OUTDIR / f\"{name}_best.joblib\")\n",
        "\n",
        "print(f\"\\nArtifacts saved to: {OUTDIR}\")"
      ]
    }
  ]
}