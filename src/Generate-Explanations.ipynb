{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c528b638-3fca-400a-abcd-0f6127cb95e1",
   "metadata": {},
   "source": [
    "## Load libraries and set up directory paths and iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccf291-05fc-4c9d-a0fc-f476227bbf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from modules.explanations import aggregate_shap_batches, generate_shap_explanations\n",
    "from modules.utils import load_dataset, load_object, save_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544c7d6-a567-44c8-aa83-e3ae89f8c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory paths for models, datasets, and SHAP explanations\n",
    "models_dir = '../models'\n",
    "datasets_dir = '../data/processed/train'\n",
    "explanations_dir = '../shap-explanations'\n",
    "\n",
    "# Define iterables for looping over datasets and model types\n",
    "dataset_names = sorted(os.listdir(models_dir))\n",
    "model_types = ['logreg', 'dt', 'rf', 'xgb', 'svc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b94698-e9a6-4c0e-91ce-8d426cf9dd57",
   "metadata": {},
   "source": [
    "## Compute global SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bdfc79-56d7-4159-afcf-0ce69b847468",
   "metadata": {},
   "source": [
    "### Full SHAP calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c7cb6-6692-4e44-9bd7-d963ccf31895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Evaluating SHAP values takes a long time for certain datasets - trained individually on Google Colab\n",
    "# For each dataset, compute explanations\n",
    "for name in dataset_names:\n",
    "    print(f'Computing SHAP explanations for {name}')\n",
    "    explanations_subdir = os.path.join(explanations_dir, name)\n",
    "\n",
    "    # If the destination directory for explanations does not exist, create it\n",
    "    if not os.path.exists(explanations_subdir):\n",
    "        print(f'Creating directory for SHAP explanations at {explanations_subdir}')\n",
    "        os.makedirs(explanations_subdir)\n",
    "\n",
    "    # Load the training set for the dataset\n",
    "    training_set_path = os.path.join(datasets_dir, name.replace('-', '_') + '-train.csv.gz')\n",
    "    print(f'Loading dataset at {training_set_path}')\n",
    "    training_set = load_dataset(training_set_path)\n",
    "\n",
    "    # Loop through the different model types\n",
    "    for type_ in model_types:\n",
    "        model_path = os.path.join(models_dir, name, name.replace('-', '_') + f'-{type_}-full.pickle')\n",
    "        print(f'Loading model at {model_path}')\n",
    "        model = load_object(model_path)\n",
    "\n",
    "        # Compute the global SHAP explanations\n",
    "        explanation_path = os.path.join(\n",
    "                explanations_subdir, name.replace('-', '_') + f\"-{type_}-sampling-global\"\n",
    "            )\n",
    "        shap_explanation = generate_shap_explanations(\n",
    "                model_eval = model.predict,\n",
    "                data = training_set,\n",
    "                random_state = 42\n",
    "            )\n",
    "        save_object(shap_explanation, explanation_path, compression = 'lzma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7966dd0-275c-4b18-8538-3873d651ea71",
   "metadata": {},
   "source": [
    "### Batch SHAP version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0348c3c3-ccf4-40f4-834a-bac330af4b27",
   "metadata": {},
   "source": [
    "Datasets and models explained using batching:\n",
    "- Credit Card Fraud - Random Forest and XGBoost\n",
    "- Patient Survival - Random Forest and XGBoost\n",
    "- Android Permissions - Random Forest\n",
    "- Phishing URL - Random Forest and XGBoost\n",
    "- Secondary Mushroom - Random Forest and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a599305c-82a9-45fb-9cbc-6b64cbb3e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Evaluating SHAP values takes a long time for certain datasets - trained individually on Google Colab\n",
    "\n",
    "# Ability to resume batches\n",
    "old_batch_stop = 0\n",
    "batches = batches[old_batch_stop:]\n",
    "\n",
    "# For each dataset, compute explanations\n",
    "for name in dataset_names:\n",
    "    print(f'Computing SHAP explanations for {name}')\n",
    "    explanations_subdir = os.path.join(explanations_dir, name)\n",
    "\n",
    "    # If the destination directory for explanations does not exist, create it\n",
    "    if not os.path.exists(explanations_subdir):\n",
    "        print(f'Creating directory for SHAP explanations at {explanations_subdir}')\n",
    "        os.makedirs(explanations_subdir)\n",
    "\n",
    "    # Load the training set for the dataset\n",
    "    training_set_path = os.path.join(datasets_dir, name.replace('-', '_') + '-train.csv.gz')\n",
    "    print(f'Loading dataset at {training_set_path}')\n",
    "    training_set = load_dataset(training_set_path)\n",
    "\n",
    "    # Loop through the different model types\n",
    "    for type_ in model_types:\n",
    "        model_path = os.path.join(models_dir, name, name.replace('-', '_') + f'-{type_}-full.pickle.xz')\n",
    "        print(f'Loading model at {model_path}')\n",
    "        model = load_object(model_path)\n",
    "\n",
    "        for i, _ in enumerate(batches[:-1]):\n",
    "            # Compute the batch SHAP explanations\n",
    "            print(f'Computing SHAP values for records {batches[i]} - {batches[i + 1]}')\n",
    "            explanation_path = os.path.join(\n",
    "                explanations_subdir, name.replace('-', '_') + f\"-{type_}-sampling-global-batch{old_batch_stop + i}\"\n",
    "            )\n",
    "            shap_explanation = generate_shap_explanations(\n",
    "                model_eval = model.predict,\n",
    "                data = training_set,\n",
    "                random_state = 42,\n",
    "                batch = slice(batches[i], batches[i + 1])\n",
    "            )\n",
    "            save_object(shap_explanation, explanation_path, compression = 'lzma')\n",
    "            \n",
    "        # Compute the last batch SHAP explanations\n",
    "        print(f'Computing SHAP values for records {batches[-1]} - {len(training_set)}')\n",
    "        explanation_path = os.path.join(\n",
    "            explanations_subdir,\n",
    "            name.replace('-', '_') + f\"-{type_}-sampling-global-batch{len(batches) - 1 + old_batch_stop}\"\n",
    "        )\n",
    "        shap_explanation = generate_shap_explanations(\n",
    "            model_eval = model.predict,\n",
    "            data = training_set,\n",
    "            random_state = 42,\n",
    "            batch = slice(batches[-1], None)\n",
    "        )\n",
    "        save_object(shap_explanation, explanation_path, compression = 'lzma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab152d-2f7c-455a-831d-e0e6c4906078",
   "metadata": {},
   "source": [
    "## Aggregate Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db06e5-7cb0-4172-830d-a59f5fb788e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating SHAP explanation batches for all datasets and model types\n",
    "for name in dataset_names:\n",
    "    # Iterate over each model type to aggregate its SHAP batches\n",
    "    for model_type in model_types:\n",
    "        try:\n",
    "            # Log the aggregation attempt for traceability\n",
    "            print(f'Attempted to aggregate SHAP batches for {name} with model {model_type}\\n')\n",
    "\n",
    "            # Aggregate all SHAP batch files for the given dataset and model type\n",
    "            aggregate_shap_batches(os.path.join(explanations_dir, name), model_type)\n",
    "\n",
    "            # Confirm successful aggregation\n",
    "            print(f'\\nSuccessfully aggregated SHAP batches for {name} with model {model_type}\\n')\n",
    "\n",
    "        except:\n",
    "            # Handle missing or inaccessible SHAP batches gracefully\n",
    "            print('No SHAP batches found\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
